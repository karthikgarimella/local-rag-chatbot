Local Q&A Chatbot using RAG
ğŸ” Project Overview

This project is a fully local Retrieval-Augmented Generation (RAG) based Question & Answer chatbot that allows users to upload a PDF document and ask questions about its content.

Unlike cloud-based solutions, this chatbot:

Runs entirely on a local machine

Uses open-source LLMs

Does not rely on any paid APIs

Answers strictly from the uploaded document, reducing hallucinations

The goal of this project is to demonstrate a clear, practical understanding of RAG architecture, vector search, and LLM grounding â€” not just tool usage.

ğŸ§  What Problem Does This Solve?

Large Language Models (LLMs) often:

Hallucinate answers

Lack access to private or domain-specific data

Require expensive cloud APIs

This project solves that by:

Retrieving relevant document chunks using embeddings + vector search

Injecting those chunks into the LLM prompt

Forcing the model to answer only from retrieved context

ğŸ—ï¸ Architecture (RAG Flow)
User Question
   â†“
Convert question to embedding
   â†“
FAISS Vector Search (Top-K relevant chunks)
   â†“
Context Injection into Prompt
   â†“
Local LLM (Ollama)
   â†“
Grounded Answer


This explicit pipeline avoids high-level black-box abstractions and keeps the system transparent and explainable.

ğŸ§° Tech Stack & Rationale
Component	Technology	Why It Was Used
LLM	Ollama (phi / tinyllama)	Fully local inference, no paid APIs
Embeddings	sentence-transformers (MiniLM)	Lightweight and fast on CPU
Vector DB	FAISS	Efficient similarity search
RAG Logic	Custom retrieval + prompt injection	Avoids unstable high-level wrappers
UI	Streamlit	Simple, fast chatbot interface
Language	Python	Strong ecosystem for GenAI
ğŸš€ Features

ğŸ“¤ Upload any PDF document

ğŸ’¬ Ask natural language questions

ğŸ§  Retrieval-Augmented Generation (true RAG)

ğŸš« No hallucinated answers (context enforced)

ğŸ–¥ï¸ Fully local execution (privacy-first)

âš¡ Optimized for CPU-only systems

â–¶ï¸ How to Run Locally
1ï¸âƒ£ Prerequisites

Python 3.10+

Ollama installed and running

Local LLM pulled (e.g. phi or tinyllama)

ollama pull phi

2ï¸âƒ£ Clone / Download Project

Place all files inside a folder, e.g.:

rag_chatbot/
â”œâ”€â”€ app.py
â”œâ”€â”€ rag_basic.py
â”œâ”€â”€ README.md

3ï¸âƒ£ Create & Activate Virtual Environment
py -m venv venv
venv\Scripts\activate

4ï¸âƒ£ Install Dependencies
pip install streamlit langchain-community langchain-text-splitters \
           sentence-transformers faiss-cpu pypdf

5ï¸âƒ£ Run the Chatbot
streamlit run app.py


Open the browser at:

http://localhost:8501


Upload a PDF and start asking questions.

ğŸ§  Key Design Decisions (Interview Focus)
Why Local LLM instead of OpenAI / GPT?

Avoids cost and rate limits

Preserves data privacy

Demonstrates real understanding of model constraints

Makes the system reproducible offline

Why Small Models Work Well Here

Because RAG supplies relevant context, the LLM:

Does not need to â€œrememberâ€ everything

Only needs to reason over retrieved text

This allows smaller models to perform well with lower resource usage.

How Hallucinations Are Reduced

Answers are restricted to retrieved document chunks

Prompt explicitly forbids external knowledge

If information is missing, the model responds with â€œI donâ€™t knowâ€

Why Not Use High-Level LangChain Wrappers?

LangChain APIs change frequently

Explicit retrieval + prompt injection is:

More stable

Easier to debug

Easier to explain in interviews

âš ï¸ Limitations

Response time is slower than cloud LLMs (CPU-only local inference)

Embeddings are rebuilt when a new PDF is uploaded

Designed for demo and learning, not production scale

ğŸ”® Future Improvements

Persistent vector store per document

Chat history / conversational memory

Support for multiple PDFs

Optional cloud deployment

UI improvements and response streaming

ğŸ¯ Intended Audience

This project is aimed at:

GenAI Engineer roles

ML / AI Engineer roles

Candidates learning RAG and LLM systems

Interview demonstrations and technical discussions

ğŸ§¾ Summary

This project demonstrates:

Practical RAG implementation

Clear understanding of GenAI system design

Ability to work with local LLMs and real constraints

Strong debugging and architectural reasoning

ğŸ“Œ Note

Response latency is expected due to fully local execution on CPU.
This is an intentional trade-off for cost, privacy, and transparency.